{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEdBmx11Yu6+JrS2Tzozjx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anobody777/RL/blob/main/%E9%AC%BC%E6%8A%93%E4%BA%BA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6r_yZ4hyOZma",
        "outputId": "5349eab8-84ec-4729-bb9b-ba410e448b7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================\n",
            "開始模擬: 獨立 RL (Greedy) 策略\n",
            "==============================\n",
            "t = 0.50s | 人: 3.50, 鬼1: 8.00, 鬼2: 7.00\n",
            "t = 1.00s | 人: 3.00, 鬼1: 7.00, 鬼2: 6.00\n",
            "t = 1.50s | 人: 2.50, 鬼1: 6.00, 鬼2: 5.00\n",
            "t = 2.00s | 人: 2.00, 鬼1: 5.00, 鬼2: 4.00\n",
            "t = 2.51s | 人: 1.49, 鬼1: 3.98, 鬼2: 2.98\n",
            "t = 3.01s | 人: 0.99, 鬼1: 2.98, 鬼2: 1.98\n",
            "t = 3.51s | 人: 0.49, 鬼1: 1.98, 鬼2: 0.98\n",
            "------------------------------\n",
            "!!! 抓到! (RL) !!!\n",
            "最終時間: t = 3.91 秒\n",
            "最終位置: 人: 0.09, 鬼1: 1.18, 鬼2: 0.18\n",
            "==============================\n",
            "\n",
            "==============================\n",
            "開始模擬: 合作 MARL (QMIX) 策略\n",
            "==============================\n",
            "t = 0.50s | 人: 3.50, 鬼1: 9.00, 鬼2: 7.00 (鬼1回城剩餘: 1.50s)\n",
            "t = 1.00s | 人: 3.00, 鬼1: 9.00, 鬼2: 6.00 (鬼1回城剩餘: 1.00s)\n",
            "t = 1.50s | 人: 2.50, 鬼1: 9.00, 鬼2: 5.00 (鬼1回城剩餘: 0.50s)\n",
            "t = 2.00s | 鬼1 (a1) 完成回城，出現在 x=0\n",
            "t = 2.00s | 人: 2.00, 鬼1: 0.00, 鬼2: 4.00 (鬼1回城剩餘: -0.00s)\n",
            "t = 2.51s | 人: 1.49, 鬼1: 1.02, 鬼2: 2.98 (鬼1回城剩餘: -0.00s)\n",
            "------------------------------\n",
            "!!! 抓到! (MARL) !!!\n",
            "最終時間: t = 2.64 秒\n",
            "最終位置: 人: 1.36, 鬼1: 1.28, 鬼2: 2.72\n",
            "==============================\n",
            "\n",
            "模擬總結：\n",
            "  獨立 RL (Greedy) 耗時: 3.91 秒\n",
            "  合作 MARL (QMIX) 耗時: 2.64 秒\n",
            "  合作策略節省了 1.27 秒\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "# --- 1. 環境設定 ---\n",
        "GHOST_SPEED = 2.0  # 鬼的速度 (單位/秒)\n",
        "HUMAN_SPEED = 1.0  # 人的速度 (單位/秒)\n",
        "RECALL_TIME = 2.0  # 鬼 \"回城\" 所需的時間 (秒)\n",
        "\n",
        "MAP_MIN = 0.0\n",
        "MAP_MAX = 10.0\n",
        "DT = 0.01          # 模擬的時間精度 (每 0.01 秒更新一次)\n",
        "CAPTURE_DISTANCE = 0.1 # 判定抓到的距離\n",
        "\n",
        "# --- 2. 模擬：獨立 RL (Greedy) 策略 ---\n",
        "# 假設：所有鬼都只會「貪婪地」追逐（即，都向左追）\n",
        "def simulate_greedy_rl():\n",
        "    print(\"=\"*30)\n",
        "    print(\"開始模擬: 獨立 RL (Greedy) 策略\")\n",
        "    print(\"=\"*30)\n",
        "\n",
        "    # 初始位置\n",
        "    b_pos = 4.0\n",
        "    a1_pos = 9.0\n",
        "    a2_pos = 8.0\n",
        "\n",
        "    # 速度 (人向左逃跑，鬼都向左追)\n",
        "    b_vel = -HUMAN_SPEED\n",
        "    a1_vel = -GHOST_SPEED\n",
        "    a2_vel = -GHOST_SPEED\n",
        "\n",
        "    t = 0.0\n",
        "\n",
        "    while t < 10.0: # 最多模擬 10 秒\n",
        "        t += DT\n",
        "\n",
        "        # 更新位置\n",
        "        b_pos += b_vel * DT\n",
        "        a1_pos += a1_vel * DT\n",
        "        a2_pos += a2_vel * DT\n",
        "\n",
        "        # 檢查地圖邊界\n",
        "        b_pos = max(MAP_MIN, min(MAP_MAX, b_pos))\n",
        "        a1_pos = max(MAP_MIN, min(MAP_MAX, a1_pos))\n",
        "        a2_pos = max(MAP_MIN, min(MAP_MAX, a2_pos))\n",
        "\n",
        "        # 每 0.5 秒印出一次狀態\n",
        "        if t % 0.5 < DT:\n",
        "            print(f\"t = {t:.2f}s | 人: {b_pos:.2f}, 鬼1: {a1_pos:.2f}, 鬼2: {a2_pos:.2f}\")\n",
        "\n",
        "        # 檢查是否抓到\n",
        "        if abs(b_pos - a1_pos) < CAPTURE_DISTANCE or abs(b_pos - a2_pos) < CAPTURE_DISTANCE:\n",
        "            print(\"-\" * 30)\n",
        "            print(f\"!!! 抓到! (RL) !!!\")\n",
        "            print(f\"最終時間: t = {t:.2f} 秒\")\n",
        "            print(f\"最終位置: 人: {b_pos:.2f}, 鬼1: {a1_pos:.2f}, 鬼2: {a2_pos:.2f}\")\n",
        "            print(\"=\"*30 + \"\\n\")\n",
        "            return t\n",
        "\n",
        "    print(\"--- RL 策略在 10 秒內未能抓到 ---\")\n",
        "    return t\n",
        "\n",
        "# --- 3. 模擬：合作 MARL (QMIX) 策略 ---\n",
        "# 假設：a1 (遠的鬼) 犧牲自己 \"回城\"，a2 (近的鬼) 負責追趕\n",
        "def simulate_cooperative_marl():\n",
        "    print(\"=\"*30)\n",
        "    print(\"開始模擬: 合作 MARL (QMIX) 策略\")\n",
        "    print(\"=\"*30)\n",
        "\n",
        "    # 初始位置\n",
        "    b_pos = 4.0\n",
        "    a1_pos = 9.0\n",
        "    a2_pos = 8.0\n",
        "\n",
        "    # 速度 (人向左逃跑，鬼2向左追)\n",
        "    b_vel = -HUMAN_SPEED\n",
        "    a2_vel = -GHOST_SPEED\n",
        "\n",
        "    # 鬼1 的狀態\n",
        "    a1_vel = 0.0 # 鬼1 正在 \"回城\"，不能移動\n",
        "    a1_recall_timer = RECALL_TIME\n",
        "\n",
        "    t = 0.0\n",
        "\n",
        "    while t < 10.0:\n",
        "        t += DT\n",
        "\n",
        "        # --- 更新 鬼1 (a1) 的狀態 ---\n",
        "        if a1_recall_timer > 0:\n",
        "            a1_recall_timer -= DT\n",
        "            if a1_recall_timer <= 0:\n",
        "                print(f\"t = {t:.2f}s | 鬼1 (a1) 完成回城，出現在 x=0\")\n",
        "                a1_pos = 0.0          # 傳送到 x=0\n",
        "                a1_vel = GHOST_SPEED  # 開始向「右」追 (形成包圍)\n",
        "        else:\n",
        "            # 鬼1 正常移動\n",
        "            a1_pos += a1_vel * DT\n",
        "\n",
        "        # --- 更新 鬼2 (a2) 和 人 (b) 的狀態 ---\n",
        "        b_pos += b_vel * DT\n",
        "        a2_pos += a2_vel * DT\n",
        "\n",
        "        # 檢查地圖邊界\n",
        "        b_pos = max(MAP_MIN, min(MAP_MAX, b_pos))\n",
        "        a1_pos = max(MAP_MIN, min(MAP_MAX, a1_pos))\n",
        "        a2_pos = max(MAP_MIN, min(MAP_MAX, a2_pos))\n",
        "\n",
        "        # 每 0.5 秒印出一次狀態\n",
        "        if t % 0.5 < DT:\n",
        "            print(f\"t = {t:.2f}s | 人: {b_pos:.2f}, 鬼1: {a1_pos:.2f}, 鬼2: {a2_pos:.2f} (鬼1回城剩餘: {a1_recall_timer:.2f}s)\")\n",
        "\n",
        "        # 檢查是否抓到\n",
        "        if abs(b_pos - a1_pos) < CAPTURE_DISTANCE or abs(b_pos - a2_pos) < CAPTURE_DISTANCE:\n",
        "            print(\"-\" * 30)\n",
        "            print(f\"!!! 抓到! (MARL) !!!\")\n",
        "            print(f\"最終時間: t = {t:.2f} 秒\")\n",
        "            print(f\"最終位置: 人: {b_pos:.2f}, 鬼1: {a1_pos:.2f}, 鬼2: {a2_pos:.2f}\")\n",
        "            print(\"=\"*30 + \"\\n\")\n",
        "            return t\n",
        "\n",
        "    print(\"--- MARL 策略在 10 秒內未能抓到 ---\")\n",
        "    return t\n",
        "\n",
        "# --- 4. 執行模擬 ---\n",
        "if __name__ == \"__main__\":\n",
        "    rl_time = simulate_greedy_rl()\n",
        "    marl_time = simulate_cooperative_marl()\n",
        "\n",
        "    print(f\"模擬總結：\")\n",
        "    print(f\"  獨立 RL (Greedy) 耗時: {rl_time:.2f} 秒\")\n",
        "    print(f\"  合作 MARL (QMIX) 耗時: {marl_time:.2f} 秒\")\n",
        "    print(f\"  合作策略節省了 {rl_time - marl_time:.2f} 秒\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 0. 安裝依賴 ---\n",
        "# (我們需要 pettingzoo 來建立環境基礎)\n",
        "!pip install pettingzoo pygame pyvirtualdisplay moviepy\n",
        "\n",
        "# --- 1. 導入 (Imports) ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque, namedtuple\n",
        "import time\n",
        "import math\n",
        "import pyvirtualdisplay\n",
        "import gym\n",
        "import imageio\n",
        "from pettingzoo.utils.env import ParallelEnv\n",
        "from gym.spaces import Discrete, Box\n",
        "from IPython.display import Image, display\n",
        "import os # 為了 Drive 儲存/加載\n",
        "\n",
        "# --- 2. 啟動虛擬螢幕 ---\n",
        "try:\n",
        "    _display.stop()\n",
        "except:\n",
        "    pass\n",
        "_display = pyvirtualdisplay.Display(visible=0, size=(500, 150))\n",
        "_display.start()\n",
        "print(\"Virtual display started.\")\n",
        "\n",
        "# --- 3. 【全新】1D 鬼抓人環境 (GhostHumanEnv) ---\n",
        "# (這模擬了您描述的 1D 遊戲)\n",
        "class GhostHumanEnv(ParallelEnv):\n",
        "    metadata = {'render.modes': ['human', 'rgb_array'], 'name': 'ghost_human_v1'}\n",
        "\n",
        "    def __init__(self, num_ghosts=2):\n",
        "        self.MAP_MIN, self.MAP_MAX = 0.0, 10.0\n",
        "        self.GHOST_SPEED = 2.0  # 單位/秒\n",
        "        self.HUMAN_SPEED = 1.0  # 單位/秒\n",
        "        self.RECALL_TIME = 2.0    # 2 秒\n",
        "        self.TIME_STEP = 0.5      # 模擬的每一步是 0.5 秒\n",
        "        self.RECALL_STEPS = int(self.RECALL_TIME / self.TIME_STEP) # = 4 步\n",
        "        self.CAPTURE_DISTANCE = 0.25 # 判定抓到的距離\n",
        "        self.MAX_STEPS = 50       # 每回合最多 50 步 (25 秒)\n",
        "\n",
        "        self.num_ghosts = num_ghosts\n",
        "        self.ghost_agent_ids = [f\"ghost_{i}\" for i in range(num_ghosts)]\n",
        "        self.human_agent_id = \"human_0\"\n",
        "        self.possible_agents = self.ghost_agent_ids + [self.human_agent_id]\n",
        "\n",
        "        # 狀態空間 (觀測) for Ghost: [human_pos, my_pos, other_ghost_pos, my_recall_timer]\n",
        "        self.observation_space_ghost = Box(low=-1.0, high=self.MAP_MAX, shape=(4,))\n",
        "        # 狀態空間 for Human: [my_pos, ghost_0_pos, ghost_1_pos]\n",
        "        self.observation_space_human = Box(low=-1.0, high=self.MAP_MAX, shape=(3,))\n",
        "\n",
        "        # 動作空間 for Ghost: 0=向左, 1=向右, 2=回城, 3=不動\n",
        "        self.action_space_ghost = Discrete(4)\n",
        "        # 動作空間 for Human: 0=向左, 1=向右 (註：在此環境中由 AI 固定控制)\n",
        "        self.action_space_human = Discrete(2)\n",
        "\n",
        "        # PettingZoo API\n",
        "        self.observation_spaces = {name: self.observation_space_ghost for name in self.ghost_agent_ids}\n",
        "        self.observation_spaces[self.human_agent_id] = self.observation_space_human\n",
        "        self.action_spaces = {name: self.action_space_ghost for name in self.ghost_agent_ids}\n",
        "        self.action_spaces[self.human_agent_id] = self.action_space_human\n",
        "\n",
        "        self.screen = None\n",
        "\n",
        "    def reset(self, seed=None, hard_code_scenario=False):\n",
        "        # hard_code_scenario 用於測試您指定的初始狀態\n",
        "        if hard_code_scenario:\n",
        "            self.human_pos = 4.0\n",
        "            self.ghost_pos = [9.0, 8.0]\n",
        "        else: # 隨機初始位置 (用於訓練)\n",
        "            self.human_pos = random.uniform(3.0, 7.0)\n",
        "            self.ghost_pos = [random.uniform(7.0, 10.0) for _ in range(self.num_ghosts)]\n",
        "\n",
        "        self.ghost_recall_timers = [0] * self.num_ghosts # 0 = 不在回城\n",
        "        self.steps = 0\n",
        "\n",
        "        # PettingZoo API: agents 列表必須是當前活躍的\n",
        "        self.agents = self.possible_agents[:] # 複製列表\n",
        "\n",
        "        return self._get_observations()\n",
        "\n",
        "    def _get_observations(self):\n",
        "        obs = {}\n",
        "        for i in range(self.num_ghosts):\n",
        "            other_ghost_pos = self.ghost_pos[1-i] # (假設只有 2 個鬼)\n",
        "            obs[f\"ghost_{i}\"] = np.array([self.human_pos, self.ghost_pos[i], other_ghost_pos, self.ghost_recall_timers[i]])\n",
        "        obs[\"human_0\"] = np.array([self.human_pos, self.ghost_pos[0], self.ghost_pos[1]])\n",
        "        return obs\n",
        "\n",
        "    def step(self, actions):\n",
        "        if not self.agents: return {}, {}, {}, {}, {} # 如果回合已結束\n",
        "\n",
        "        rewards = {name: 0.0 for name in self.agents}\n",
        "        terminations = {name: False for name in self.agents}\n",
        "\n",
        "        # --- 1. 人類 AI (固定策略：往左逃跑) ---\n",
        "        if self.human_pos > self.MAP_MIN:\n",
        "            self.human_pos += (-self.HUMAN_SPEED * self.TIME_STEP)\n",
        "        self.human_pos = max(self.MAP_MIN, self.human_pos)\n",
        "\n",
        "        # --- 2. 鬼的行動 (掠食者) ---\n",
        "        for i in range(self.num_ghosts):\n",
        "            ghost_name = f\"ghost_{i}\"\n",
        "            action = actions.get(ghost_name, 3) # 默認為 3 (不動)\n",
        "\n",
        "            # 檢查是否正在回城\n",
        "            if self.ghost_recall_timers[i] > 0:\n",
        "                self.ghost_recall_timers[i] -= 1\n",
        "                if self.ghost_recall_timers[i] == 0:\n",
        "                    self.ghost_pos[i] = self.MAP_MIN # 完成回城，瞬移到 0\n",
        "\n",
        "            # 如果不在回城，則執行動作\n",
        "            elif action == 0: # 向左\n",
        "                self.ghost_pos[i] -= self.GHOST_SPEED * self.TIME_STEP\n",
        "            elif action == 1: # 向右\n",
        "                self.ghost_pos[i] += self.GHOST_SPEED * self.TIME_STEP\n",
        "            elif action == 2: # 啟動回城\n",
        "                self.ghost_recall_timers[i] = self.RECALL_STEPS\n",
        "            # action == 3 (不動)\n",
        "\n",
        "            self.ghost_pos[i] = max(self.MAP_MIN, min(self.MAP_MAX, self.ghost_pos[i]))\n",
        "\n",
        "        # --- 3. 計算獎勵和終止 ---\n",
        "        self.steps += 1\n",
        "\n",
        "        captured = False\n",
        "        for i in range(self.num_ghosts):\n",
        "            if abs(self.ghost_pos[i] - self.human_pos) < self.CAPTURE_DISTANCE:\n",
        "                captured = True; break\n",
        "\n",
        "        # 為鬼 (ghost) 和人 (human) 計算不同的獎勵\n",
        "        team_reward_ghost = 0.0  # 這是給 QMIX 和 IQL 的獎勵\n",
        "        human_reward = 0.0       # 這是給人類的獎勵 (雖然我們沒訓練它)\n",
        "\n",
        "        if captured:\n",
        "            team_reward_ghost = 100.0  # 鬼抓到人，獲得巨大獎勵\n",
        "            human_reward = -100.0\n",
        "            terminations = {name: True for name in self.agents}\n",
        "            self.agents = []\n",
        "        elif self.steps >= self.MAX_STEPS:\n",
        "            team_reward_ghost = -100.0 # 鬼超時，獲得巨大懲罰\n",
        "            human_reward = 100.0  # 人類成功逃脫\n",
        "            terminations = {name: True for name in self.agents}\n",
        "            self.agents = []\n",
        "        else:\n",
        "            # 獎勵塑形：負的距離總和 (鼓勵靠近)\n",
        "            team_reward_ghost = -sum(abs(self.ghost_pos[i] - self.human_pos) for i in range(self.num_ghosts))\n",
        "            human_reward = -team_reward_ghost # 鼓勵遠離\n",
        "\n",
        "        # 分配獎勵\n",
        "        for name in self.agents:\n",
        "            if \"ghost\" in name:\n",
        "                rewards[name] = team_reward_ghost\n",
        "            else:\n",
        "                rewards[name] = human_reward\n",
        "\n",
        "        return self._get_observations(), rewards, terminations, {name: False for name in self.agents}, {}\n",
        "\n",
        "    def render(self, mode='rgb_array'):\n",
        "        import pygame\n",
        "        if self.screen is None: pygame.init(); self.screen = pygame.display.set_mode((self.screen_width, self.screen_height))\n",
        "        self.screen.fill((255, 255, 255))\n",
        "        map_y = 75\n",
        "        map_start_x = int(self.MAP_MIN / self.MAP_MAX * self.screen_width * 0.9 + 0.05 * self.screen_width)\n",
        "        map_end_x = int(self.MAP_MAX / self.MAP_MAX * self.screen_width * 0.9 + 0.05 * self.screen_width)\n",
        "        pygame.draw.line(self.screen, (0, 0, 0), (map_start_x, map_y), (map_end_x, map_y), 2)\n",
        "        def get_x_pos(pos): return int(pos / self.MAP_MAX * (map_end_x - map_start_x) + map_start_x)\n",
        "        pygame.draw.circle(self.screen, (0, 255, 0), (get_x_pos(self.human_pos), map_y), 10)\n",
        "        for i in range(self.num_ghosts):\n",
        "            color = (255, 0, 0) if self.ghost_recall_timers[i] == 0 else (255, 180, 180)\n",
        "            pygame.draw.circle(self.screen, color, (get_x_pos(self.ghost_pos[i]), map_y), 12)\n",
        "        if mode == 'rgb_array': return np.transpose(np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2))\n",
        "        else: pygame.display.flip()\n",
        "\n",
        "    def close(self):\n",
        "        if self.screen is not None:\n",
        "            import pygame; pygame.display.quit(); pygame.quit(); self.screen = None\n",
        "\n",
        "# --- 4. 超參數 (Hyperparameters) ---\n",
        "BUFFER_SIZE, BATCH_SIZE, GAMMA, LR, TAU = int(1e5), 64, 0.95, 1e-3, 0.005\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(f\"Using device: {DEVICE}\")\n",
        "NUM_EPISODES, MAX_T = 1000, 50\n",
        "EPS_START, EPS_END, EPS_DECAY = 1.0, 0.05, 0.995\n",
        "MODEL_CHECKPOINT = 100\n",
        "MIXER_EMBED_DIM = 32\n",
        "VISUALIZATION_SEED = 5\n",
        "LOAD_EPISODE = 0\n",
        "DRIVE_PATH = '/content/gdrive/MyDrive/MARL_GhostGame_Training/' # (可以設為您的 Drive 路徑)\n",
        "\n",
        "\n",
        "# --- 5. 演算法組件 (IQL 和 QMIX) ---\n",
        "Experience = namedtuple(\"Experience\", field_names=[\"states\", \"actions\", \"reward\", \"next_states\", \"done\", \"global_state\", \"next_global_state\"])\n",
        "class SharedReplayBuffer:\n",
        "    def __init__(self, buffer_size, batch_size): self.memory, self.batch_size = deque(maxlen=buffer_size), batch_size\n",
        "    def push(self, states, actions, reward, next_states, done, global_state, next_global_state): self.memory.append(Experience(states, actions, reward, next_states, done, global_state, next_global_state))\n",
        "    def sample(self, num_agents):\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "        states = [torch.from_numpy(np.vstack([e.states[i] for e in experiences])).float().to(DEVICE) for i in range(num_agents)]\n",
        "        actions = [torch.from_numpy(np.vstack([e.actions[i] for e in experiences])).long().to(DEVICE) for i in range(num_agents)]\n",
        "        next_states = [torch.from_numpy(np.vstack([e.next_states[i] for e in experiences])).float().to(DEVICE) for i in range(num_agents)]\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences])).float().to(DEVICE)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences]).astype(np.uint8)).float().to(DEVICE)\n",
        "        global_states = torch.from_numpy(np.vstack([e.global_state for e in experiences])).float().to(DEVICE)\n",
        "        next_global_states = torch.from_numpy(np.vstack([e.next_global_state for e in experiences])).float().to(DEVICE)\n",
        "        return (states, actions, rewards, next_states, dones, global_states, next_global_states)\n",
        "    def __len__(self): return len(self.memory)\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, seed):\n",
        "        super(QNetwork, self).__init__(); self.seed = torch.manual_seed(seed)\n",
        "        self.fc1, self.fc2, self.fc3 = nn.Linear(state_dim, 32), nn.Linear(32, 32), nn.Linear(32, action_dim)\n",
        "    def forward(self, state): return self.fc3(F.relu(self.fc2(F.relu(self.fc1(state)))))\n",
        "\n",
        "class QMixer(nn.Module):\n",
        "    def __init__(self, num_agents, global_state_dim, embed_dim):\n",
        "        super(QMixer, self).__init__(); self.num_agents, self.global_state_dim, self.embed_dim = num_agents, global_state_dim, embed_dim\n",
        "        self.hyper_w1, self.hyper_b1 = nn.Linear(global_state_dim, embed_dim * num_agents), nn.Linear(global_state_dim, embed_dim)\n",
        "        self.hyper_w2 = nn.Linear(global_state_dim, embed_dim)\n",
        "        self.hyper_b2 = nn.Sequential(nn.Linear(global_state_dim, embed_dim), nn.ReLU(), nn.Linear(embed_dim, 1))\n",
        "    def forward(self, agent_q_values, global_state):\n",
        "        w1, b1 = torch.abs(self.hyper_w1(global_state)).view(-1, self.num_agents, self.embed_dim), self.hyper_b1(global_state).view(-1, 1, self.embed_dim)\n",
        "        w2, b2 = torch.abs(self.hyper_w2(global_state)).view(-1, self.embed_dim, 1), self.hyper_b2(global_state).view(-1, 1, 1)\n",
        "        agent_q_values = agent_q_values.view(-1, 1, self.num_agents)\n",
        "        q_total = torch.bmm(F.elu(torch.bmm(agent_q_values, w1) + b1), w2) + b2\n",
        "        return q_total.view(-1, 1)\n",
        "\n",
        "# IQL Manager (對照組)\n",
        "class IQLManager:\n",
        "    def __init__(self, state_dims, action_dims, agent_ids, seed):\n",
        "        self.agent_ids, self.num_agents = agent_ids, len(agent_ids)\n",
        "        self.action_dims_dict = action_dims\n",
        "        self.agents = {}\n",
        "        for i, agent_id in enumerate(self.agent_ids):\n",
        "            self.agents[agent_id] = DQNAgent(state_dims[agent_id], action_dims[agent_id], seed + i)\n",
        "    def select_actions(self, obs, epsilon=0.):\n",
        "        actions = {}\n",
        "        for id, agent in self.agents.items():\n",
        "            actions[id] = agent.select_action(obs[id], epsilon)\n",
        "        return actions\n",
        "    def push_and_learn(self, s, a, r, ns, d):\n",
        "        for id, agent in self.agents.items():\n",
        "            # IQL 學習：只使用「個人」獎勵\n",
        "            individual_reward = r[id] # (環境已經計算好 team_reward_ghost)\n",
        "            agent.push_and_learn(s[id], a[id], individual_reward, ns[id], d[id])\n",
        "    def get_manager(self): return self\n",
        "    def save_model(self, drive_path, episode_num): # IQL 的儲存\n",
        "        model_dir = os.path.join(drive_path, f'IQL_episode_{episode_num}'); os.makedirs(model_dir, exist_ok=True)\n",
        "        for id, agent in self.agents.items():\n",
        "            torch.save(agent.qnetwork_local.state_dict(), os.path.join(model_dir, f'qnet_{id}.pth'))\n",
        "        print(f\"\\n--- IQL Model saved to {model_dir} ---\")\n",
        "    def load_model(self, drive_path, episode_num): # IQL 的加載\n",
        "        model_dir = os.path.join(drive_path, f'IQL_episode_{episode_num}')\n",
        "        if not os.path.exists(model_dir): return False\n",
        "        print(f\"--- Loading IQL model from {model_dir} ---\")\n",
        "        for id, agent in self.agents.items():\n",
        "            agent.qnetwork_local.load_state_dict(torch.load(os.path.join(model_dir, f'qnet_{id}.pth'), map_location=DEVICE))\n",
        "            agent.soft_update_target_networks(1.0)\n",
        "        return True\n",
        "\n",
        "# DQNAgent (IQL 的組件)\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, seed):\n",
        "        self.action_dim = action_dim\n",
        "        self.qnetwork_local = QNetwork(state_dim, action_dim, seed).to(DEVICE)\n",
        "        self.qnetwork_target = QNetwork(state_dim, action_dim, seed).to(DEVICE)\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
        "        self.memory = SharedReplayBuffer(BUFFER_SIZE, BATCH_SIZE)\n",
        "        self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
        "    def select_action(self, state, epsilon=0.):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(DEVICE)\n",
        "        self.qnetwork_local.eval();\n",
        "        with torch.no_grad(): action_values = self.qnetwork_local(state)\n",
        "        self.qnetwork_local.train()\n",
        "        if random.random() > epsilon: return np.argmax(action_values.cpu().data.numpy())\n",
        "        else: return random.choice(np.arange(self.action_dim))\n",
        "    def push_and_learn(self, s, a, r, ns, d):\n",
        "        self.memory.push((s,), (a,), r, (ns,), d, (s,), (ns,))\n",
        "        if len(self.memory) > BATCH_SIZE: self.learn(self.memory.sample(1), GAMMA)\n",
        "    def learn(self, experiences, gamma):\n",
        "        states, actions, rewards, next_states, dones, _, _ = experiences\n",
        "        s, a, ns = states[0], actions[0], next_states[0]\n",
        "        Q_targets_next = self.qnetwork_target(ns).detach().max(1)[0].unsqueeze(1)\n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "        Q_expected = self.qnetwork_local(s).gather(1, a)\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "        self.optimizer.zero_grad(); loss.backward(); self.optimizer.step()\n",
        "        self.soft_update_target_networks(TAU)\n",
        "    def soft_update_target_networks(self, tau):\n",
        "        for target_param, local_param in zip(self.qnetwork_target.parameters(), self.qnetwork_local.parameters()):\n",
        "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
        "\n",
        "# QMIX Manager (實驗組)\n",
        "class QMIXManager:\n",
        "    def __init__(self, state_dims, action_dims, global_state_dim, agent_ids, seed):\n",
        "        self.agent_ids, self.num_agents, self.action_dims_dict, self.global_state_dim = agent_ids, len(agent_ids), action_dims, global_state_dim\n",
        "        self.qnetworks_local = [QNetwork(state_dims[id], action_dims[id], seed + i).to(DEVICE) for i, id in enumerate(agent_ids)]\n",
        "        self.qnetworks_target = [QNetwork(state_dims[id], action_dims[id], seed + i).to(DEVICE) for i, id in enumerate(agent_ids)]\n",
        "        self.mixer_local = QMixer(self.num_agents, global_state_dim, MIXER_EMBED_DIM).to(DEVICE)\n",
        "        self.mixer_target = QMixer(self.num_agents, global_state_dim, MIXER_EMBED_DIM).to(DEVICE)\n",
        "        self.optimizer = optim.Adam([p for net in self.qnetworks_local for p in net.parameters()] + list(self.mixer_local.parameters()), lr=LR)\n",
        "        self.memory = SharedReplayBuffer(BUFFER_SIZE, BATCH_SIZE)\n",
        "        for local, target in zip(self.qnetworks_local, self.qnetworks_target): target.load_state_dict(local.state_dict())\n",
        "        self.mixer_target.load_state_dict(self.mixer_local.state_dict())\n",
        "    def get_manager(self): return self\n",
        "    def select_actions(self, obs, epsilon=0.):\n",
        "        actions = {}\n",
        "        for i, id in enumerate(self.agent_ids):\n",
        "            state = torch.from_numpy(obs[id]).float().unsqueeze(0).to(DEVICE); self.qnetworks_local[i].eval()\n",
        "            with torch.no_grad(): actions[id] = np.argmax(self.qnetworks_local[i](state).cpu().data.numpy())\n",
        "            self.qnetworks_local[i].train()\n",
        "            if random.random() <= epsilon: actions[id] = random.choice(np.arange(self.action_dims_dict[id]))\n",
        "        return actions\n",
        "    def push_and_learn(self, s, a, r, ns, d):\n",
        "        s_tuple = tuple(s[id] for id in self.agent_ids); a_tuple = tuple(a[id] for id in self.agent_ids)\n",
        "        ns_tuple = tuple(ns[id] for id in self.agent_ids)\n",
        "        team_reward = r[self.agent_ids[0]] # 所有鬼的獎勵都相同\n",
        "        team_done = any(d[id] for id in self.agent_ids)\n",
        "        # global_state: [a1_pos, a2_pos, b_pos, a1_timer, a2_timer]\n",
        "        global_state = np.array([s[\"ghost_0\"][1], s[\"ghost_1\"][1], s[\"human_0\"][0], s[\"ghost_0\"][3], s[\"ghost_1\"][3]])\n",
        "        next_global_state = np.array([ns[\"ghost_0\"][1], ns[\"ghost_1\"][1], ns[\"human_0\"][0], ns[\"ghost_0\"][3], ns[\"ghost_1\"][3]])\n",
        "        self.memory.push(s_tuple, a_tuple, team_reward, ns_tuple, team_done, global_state, next_global_state)\n",
        "        if len(self.memory) > BATCH_SIZE: self.learn(self.memory.sample(self.num_agents), GAMMA)\n",
        "    def learn(self, experiences, gamma):\n",
        "        states, actions, rewards, next_states, dones, global_states, next_global_states = experiences\n",
        "        Q_targets_next_total = self.mixer_target(torch.cat([self.qnetworks_target[i](next_states[i]).detach().max(1)[0].unsqueeze(1) for i in range(self.num_agents)], dim=1), next_global_states)\n",
        "        Q_targets_total = rewards + (gamma * Q_targets_next_total * (1 - dones))\n",
        "        Q_expected_total = self.mixer_local(torch.cat([self.qnetworks_local[i](states[i]).gather(1, actions[i]) for i in range(self.num_agents)], dim=1), global_states)\n",
        "        loss = F.mse_loss(Q_expected_total, Q_targets_total)\n",
        "        self.optimizer.zero_grad(); loss.backward(); self.optimizer.step()\n",
        "        self.soft_update_target_networks(TAU)\n",
        "    def soft_update_target_networks(self, tau):\n",
        "        for local, target in zip(self.qnetworks_local, self.qnetworks_target):\n",
        "            for target_param, local_param in zip(target.parameters(), local.parameters()): target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
        "        for target_param, local_param in zip(self.mixer_target.parameters(), self.mixer_local.parameters()): target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
        "    def save_model(self, drive_path, episode_num): # QMIX 的儲存\n",
        "        model_dir = os.path.join(drive_path, f'QMIX_episode_{episode_num}'); os.makedirs(model_dir, exist_ok=True)\n",
        "        for i, net in enumerate(self.qnetworks_local):\n",
        "            torch.save(net.state_dict(), os.path.join(model_dir, f'qnet_agent_{i}.pth'))\n",
        "        torch.save(self.mixer_local.state_dict(), os.path.join(model_dir, 'mixer.pth')); print(f\"\\n--- QMIX Model saved to {model_dir} ---\")\n",
        "    def load_model(self, drive_path, episode_num): # QMIX 的加載\n",
        "        model_dir = os.path.join(drive_path, f'QMIX_episode_{episode_num}')\n",
        "        if not os.path.exists(model_dir): return False\n",
        "        print(f\"--- Loading QMIX model from {model_dir} ---\")\n",
        "        for i, net in enumerate(self.qnetworks_local):\n",
        "            net.load_state_dict(torch.load(os.path.join(model_dir, f'qnet_agent_{i}.pth'), map_location=DEVICE))\n",
        "        self.mixer_local.load_state_dict(torch.load(os.path.join(model_dir, 'mixer.pth'), map_location=DEVICE))\n",
        "        self.soft_update_target_networks(1.0); return True\n",
        "\n",
        "\n",
        "# --- 7. 訓練主迴圈 ---\n",
        "def train_marl(manager_class, load_episode=0):\n",
        "    print(f\"\\n--- 開始訓練 {manager_class.__name__} ---\")\n",
        "\n",
        "    env = GhostHumanEnv(num_ghosts=2)\n",
        "    predator_ids = [agent for agent in env.agents if 'ghost' in agent]\n",
        "    state_dims = {id: env.observation_space(id).shape[0] for id in predator_ids}\n",
        "    action_dims = {id: env.action_space(id).n for id in predator_ids}\n",
        "\n",
        "    if manager_class == QMIXManager:\n",
        "        global_state_dim = 5\n",
        "        marl_manager = QMIXManager(state_dims, action_dims, global_state_dim, predator_ids, seed=42)\n",
        "    else: # IQLManager\n",
        "        marl_manager = IQLManager(state_dims, action_dims, predator_ids, seed=42)\n",
        "\n",
        "    model_loaded = False\n",
        "    if load_episode > 0:\n",
        "        # (確保 Colab 儲存格 1 已經執行)\n",
        "        try:\n",
        "            model_loaded = marl_manager.load_model(DRIVE_PATH, load_episode)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model, Google Drive might not be connected: {e}\")\n",
        "\n",
        "    if model_loaded:\n",
        "        print(f\"--- Model loaded successfully! Resuming training. ---\")\n",
        "        epsilon = EPS_END # 假設加載的模型已訓練好\n",
        "    else:\n",
        "        print(f\"--- No model loaded. Starting training from scratch. ---\")\n",
        "        epsilon = EPS_START\n",
        "\n",
        "    print(f\"Starting Training from Episode {load_episode + 1}...\")\n",
        "    start_time = time.time()\n",
        "    scores_window = deque(maxlen=100)\n",
        "    capture_window = deque(maxlen=100)\n",
        "\n",
        "    for i_episode in range(load_episode + 1, NUM_EPISODES + 1):\n",
        "        observations = env.reset()\n",
        "        captured_this_episode = False\n",
        "\n",
        "        for t in range(MAX_T):\n",
        "            actions = marl_manager.select_actions(observations, epsilon)\n",
        "            next_observations, rewards, terminations, truncations, dones = env.step(actions)\n",
        "\n",
        "            team_done = any(terminations.values())\n",
        "            if team_done and t < (MAX_T - 1): # 如果不是因為超時\n",
        "                captured_this_episode = True\n",
        "\n",
        "            marl_manager.push_and_learn(observations, actions, rewards, next_observations, terminations)\n",
        "            observations = next_observations\n",
        "\n",
        "            if team_done or all(truncations.values()):\n",
        "                break\n",
        "\n",
        "        capture_window.append(1 if captured_this_episode else 0)\n",
        "        epsilon = max(EPS_END, EPS_DECAY * epsilon)\n",
        "        # 只記錄鬼的（團隊）獎勵\n",
        "        scores_window.append(rewards[predator_ids[0]] if predator_ids[0] in rewards else 0)\n",
        "\n",
        "        if i_episode % MODEL_CHECKPOINT == 0:\n",
        "            avg_score = np.mean(scores_window)\n",
        "            capture_rate = np.mean(capture_window) * 100\n",
        "            print(f'\\rEpisode {i_episode}\\tAvg Score: {avg_score:.2f}\\tCapture Rate: {capture_rate:.1f}%\\tEpsilon: {epsilon:.3f}')\n",
        "            # 儲存檢查點 (確保 Colab 儲存格 1 已經執行)\n",
        "            try:\n",
        "                marl_manager.save_model(DRIVE_PATH, i_episode)\n",
        "            except Exception as e:\n",
        "                print(f\"Error saving model, Google Drive might not be connected: {e}\")\n",
        "\n",
        "    print(f\"\\n{manager_class.__name__} 訓練完成 in {(time.time() - start_time) / 60:.2f} minutes.\")\n",
        "    env.close()\n",
        "    return marl_manager.get_manager()\n",
        "\n",
        "# --- 8. 視覺化測試 ---\n",
        "def run_test(manager, gif_filename):\n",
        "    print(f\"\\n[Test] 測試 {manager.__class__.__name__} (b=4, a1=9, a2=8)...\")\n",
        "    vis_env = GhostHumanEnv(num_ghosts=2)\n",
        "    observations = vis_env.reset(hard_code_scenario=True)\n",
        "    frames, captured = [], False\n",
        "    predator_ids = [agent for agent in vis_env.agents if 'ghost' in agent]\n",
        "\n",
        "    for t in range(MAX_T):\n",
        "        frames.append(vis_env.render())\n",
        "        actions = manager.select_actions(observations, epsilon=0.0)\n",
        "        next_observations, rewards, terminations, truncations, infos = vis_env.step(actions)\n",
        "        observations = next_observations\n",
        "\n",
        "        if any(terminations.values()):\n",
        "            if t < (MAX_T - 1):\n",
        "                print(f\"  > 成功! 在第 {t+1} 步 ({(t+1)*vis_env.TIME_STEP:.2f} 秒) 抓到!\")\n",
        "                captured = True\n",
        "            else:\n",
        "                print(f\"  > 失敗. 在第 {t+1} 步 ({(t+1)*vis_env.TIME_STEP:.2f} 秒) 超時.\")\n",
        "            break\n",
        "\n",
        "    vis_env.close()\n",
        "    imageio.mimsave(gif_filename, frames, fps=10)\n",
        "    duration_sec = len(frames) / 10.0\n",
        "    print(f\"  > 測試 GIF 已儲存: {gif_filename} ({duration_sec:.2f} 秒)\")\n",
        "    return captured, duration_sec\n",
        "\n",
        "# --- 9. 主執行緒 ---\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # 決定是否加載\n",
        "    # LOAD_EPISODE = 0 # (從頭訓練)\n",
        "    LOAD_EPISODE = 0 # (假設您想從 1000 回合的檢查點加載)\n",
        "\n",
        "    # 訓練 IQL\n",
        "    iql_model = train_marl(IQLManager, LOAD_EPISODE)\n",
        "\n",
        "    # 訓練 QMIX\n",
        "    qmix_model = train_marl(QMIXManager, LOAD_EPISODE)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 40)\n",
        "    print(\" 最終視覺化比較 \".center(40, \"=\"))\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # 測試 IQL\n",
        "    iql_result = run_test(iql_model, \"ghost_game_IQL.gif\")\n",
        "\n",
        "    # 測試 QMIX\n",
        "    qmix_result = run_test(qmix_model, \"ghost_game_QMIX.gif\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 40)\n",
        "    print(\" 最終結果 \".center(40, \"=\"))\n",
        "    print(f\"  獨立 RL (IQL):   {'成功' if iql_result[0] else '失敗'} (耗時 {iql_result[1]:.2f} 秒)\")\n",
        "    print(f\"  合作 MARL (QMIX): {'成功' if qmix_result[0] else '失敗'} (耗時 {qmix_result[1]:.2f} 秒)\")\n",
        "    print(\"=\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pnOEsOO2RhkQ",
        "outputId": "749fd479-e7a7-470a-81c4-ec934a5bbf62"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pettingzoo\n",
            "  Downloading pettingzoo-1.25.0-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.12/dist-packages (2.6.1)\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.12/dist-packages (1.0.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from pettingzoo) (2.0.2)\n",
            "Requirement already satisfied: gymnasium>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from pettingzoo) (1.2.1)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.12/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.12/dist-packages (from moviepy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.32.4)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.12/dist-packages (from moviepy) (0.1.12)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.37.0)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from moviepy) (0.6.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=1.0.0->pettingzoo) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=1.0.0->pettingzoo) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=1.0.0->pettingzoo) (0.0.4)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.12/dist-packages (from imageio<3.0,>=2.5->moviepy) (11.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2025.10.5)\n",
            "Downloading pettingzoo-1.25.0-py3-none-any.whl (852 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m852.5/852.5 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay, pettingzoo\n",
            "Successfully installed pettingzoo-1.25.0 pyvirtualdisplay-3.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Virtual display started.\n",
            "Using device: cpu\n",
            "\n",
            "--- 開始訓練 IQLManager ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'GhostHumanEnv' object has no attribute 'agents'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3810974732.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;31m# 訓練 IQL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m     \u001b[0miql_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_marl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIQLManager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLOAD_EPISODE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;31m# 訓練 QMIX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3810974732.py\u001b[0m in \u001b[0;36mtrain_marl\u001b[0;34m(manager_class, load_episode)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGhostHumanEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_ghosts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m     \u001b[0mpredator_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'ghost'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m     \u001b[0mstate_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredator_ids\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0maction_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredator_ids\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'GhostHumanEnv' object has no attribute 'agents'"
          ]
        }
      ]
    }
  ]
}